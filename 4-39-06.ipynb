{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":74,"outputs":[{"output_type":"stream","text":"/kaggle/input/movie-lens-small-latest-dataset/ratings.csv\n/kaggle/input/movie-lens-small-latest-dataset/links.csv\n/kaggle/input/movie-lens-small-latest-dataset/tags.csv\n/kaggle/input/movie-lens-small-latest-dataset/README.txt\n/kaggle/input/movie-lens-small-latest-dataset/movies.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install pyspark\nimport pyspark","execution_count":75,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.6/site-packages (2.4.3)\r\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/lib/python3.6/site-packages (from pyspark) (0.10.7)\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Building a Recommendation System in PySpark - Lab\n\n## Introduction\n\nIn this last lab, we will implement a a movie recommendation system using ALS in Spark programming environment. Spark's machine learning libraray `ml` comes packaged with a very efficient imeplementation of ALS algorithm that we looked at in the previous lesson. The lab will require you to put into pratice your spark programming skills for creating and manipulating pyspark DataFrames. We will go through a step-by-step process into developing a movie recommendation system using ALS and pyspark using the MovieLens Dataset that we used in a previous lab.\n\nNote: You are advised to refer to [PySpark Documentation](http://spark.apache.org/docs/2.2.0/api/python/index.html) heavily for completing this lab as it will introduce a few new methods. \n\n\n## Objectives\n\nYou will be able to:\n\n* Demonstrate an understanding on how recommendation systems are being used for personalization of online services/products\n* Parse and filter datasets into Spark RDDs, performing basic feature selection\n* Run a brief hyper-parameter selection activity through a scalable grid search\n* Train and evaluate the predictive performance of recommendation system\n* Generate predictions from the trained model"},{"metadata":{},"cell_type":"markdown","source":"## Building a Recommendation System\n\nWe have seen how recommender/Recommendation Systems have played an  integral parts in the success of Amazon (Books, Items), Pandora/Spotify (Music), Google (News, Search), YouTube (Videos) etc.  For Amazon these systems bring more than 30% of their total revenues. For Netflix service, 75% of movies that people watch are based on some sort of recommendation.\n\n> The goal of Recommendation Systems is to find what is likely to be of interest to the user. This enables organizations to offer a high level of personalization and customer tailored services.\n\n\nFor online video content services like Netflix and Hulu, the need to build robust movie recommendation systems is extremely important. An example of recommendation system is such as this:\n\n1.    User A watches Game of Thrones and Breaking Bad.\n2.    User B performs a search query for Game of Thrones.\n3.    The system suggests Breaking Bad to user B from data collected about user A.\n\n\nThis lab will guide you through a step-by-step process into developing such a movie recommendation system. We will use the MovieLens dataset to build a movie recommendation system using the collaborative filtering technique with Spark's Alternating Least Saqures implementation. After building that recommendation system, we will go through the process of adding a new user to the dataset with some new ratings and obtaining new recommendations for that user.\n\n### Importing the Data\nTo begin with:\n* initialize a SparkSession object\n* import the dataset found at './data/ratings.csv' into a pyspark DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary libraries\nfrom pyspark.sql import SparkSession\n\n\n# instantiate SparkSession object\nspark = SparkSession.builder.master(\"local\").getOrCreate()\n\n","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a path variable for data \nfile = '/kaggle/input/movie-lens-small-latest-dataset/ratings.csv'\n#Â Code here \nfile","execution_count":77,"outputs":[{"output_type":"execute_result","execution_count":77,"data":{"text/plain":"'/kaggle/input/movie-lens-small-latest-dataset/ratings.csv'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the dataset into pyspark DataFrame\nmovie_ratings = spark.read.csv(file, header = 'true', inferSchema = 'true')","execution_count":111,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the data types of each of the values to ensure that they are a type that makes sense given the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_ratings.dtypes","execution_count":79,"outputs":[{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"[('userId', 'int'),\n ('movieId', 'int'),\n ('rating', 'double'),\n ('timestamp', 'int')]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_ratings.head(5)","execution_count":80,"outputs":[{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"[Row(userId=1, movieId=1, rating=4.0, timestamp=964982703),\n Row(userId=1, movieId=3, rating=4.0, timestamp=964981247),\n Row(userId=1, movieId=6, rating=4.0, timestamp=964982224),\n Row(userId=1, movieId=47, rating=5.0, timestamp=964983815),\n Row(userId=1, movieId=50, rating=5.0, timestamp=964982931)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We aren't going to need the time stamp, so we can go ahead and remove that column."},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_ratings = movie_ratings.drop('timestamp')","execution_count":127,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the Alternating Least Squares Model"},{"metadata":{},"cell_type":"markdown","source":"Because this dataset is already preprocessed for us, we can go ahead and fit the Alternating Least Squares model.\n\n* Import the ALS module from pyspark.ml.recommendation.\n* Use the randomSplit method on the pyspark DataFrame to separate the dataset into a training and test set\n* Fit the Alternating Least Squares Model to the training dataset. Make sure to set the userCol, itemCol, and ratingCol to the appropriate names given this dataset. Then fit the data to the training set and assign it to a variable model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import RegressionEvaluator\n\nfrom pyspark.ml.recommendation import ALS\n\nhelp(ALS())","execution_count":82,"outputs":[{"output_type":"stream","text":"Help on ALS in module pyspark.ml.recommendation object:\n\nclass ALS(pyspark.ml.wrapper.JavaEstimator, pyspark.ml.param.shared.HasCheckpointInterval, pyspark.ml.param.shared.HasMaxIter, pyspark.ml.param.shared.HasPredictionCol, pyspark.ml.param.shared.HasRegParam, pyspark.ml.param.shared.HasSeed, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n |  Alternating Least Squares (ALS) matrix factorization.\n |  \n |  ALS attempts to estimate the ratings matrix `R` as the product of\n |  two lower-rank matrices, `X` and `Y`, i.e. `X * Yt = R`. Typically\n |  these approximations are called 'factor' matrices. The general\n |  approach is iterative. During each iteration, one of the factor\n |  matrices is held constant, while the other is solved for using least\n |  squares. The newly-solved factor matrix is then held constant while\n |  solving for the other factor matrix.\n |  \n |  This is a blocked implementation of the ALS factorization algorithm\n |  that groups the two sets of factors (referred to as \"users\" and\n |  \"products\") into blocks and reduces communication by only sending\n |  one copy of each user vector to each product block on each\n |  iteration, and only for the product blocks that need that user's\n |  feature vector. This is achieved by pre-computing some information\n |  about the ratings matrix to determine the \"out-links\" of each user\n |  (which blocks of products it will contribute to) and \"in-link\"\n |  information for each product (which of the feature vectors it\n |  receives from each user block it will depend on). This allows us to\n |  send only an array of feature vectors between each user block and\n |  product block, and have the product block find the users' ratings\n |  and update the products based on these messages.\n |  \n |  For implicit preference data, the algorithm used is based on\n |  `\"Collaborative Filtering for Implicit Feedback Datasets\",\n |  <http://dx.doi.org/10.1109/ICDM.2008.22>`_, adapted for the blocked\n |  approach used here.\n |  \n |  Essentially instead of finding the low-rank approximations to the\n |  rating matrix `R`, this finds the approximations for a preference\n |  matrix `P` where the elements of `P` are 1 if r > 0 and 0 if r <= 0.\n |  The ratings then act as 'confidence' values related to strength of\n |  indicated user preferences rather than explicit ratings given to\n |  items.\n |  \n |  >>> df = spark.createDataFrame(\n |  ...     [(0, 0, 4.0), (0, 1, 2.0), (1, 1, 3.0), (1, 2, 4.0), (2, 1, 1.0), (2, 2, 5.0)],\n |  ...     [\"user\", \"item\", \"rating\"])\n |  >>> als = ALS(rank=10, maxIter=5, seed=0)\n |  >>> model = als.fit(df)\n |  >>> model.rank\n |  10\n |  >>> model.userFactors.orderBy(\"id\").collect()\n |  [Row(id=0, features=[...]), Row(id=1, ...), Row(id=2, ...)]\n |  >>> test = spark.createDataFrame([(0, 2), (1, 0), (2, 0)], [\"user\", \"item\"])\n |  >>> predictions = sorted(model.transform(test).collect(), key=lambda r: r[0])\n |  >>> predictions[0]\n |  Row(user=0, item=2, prediction=-0.13807615637779236)\n |  >>> predictions[1]\n |  Row(user=1, item=0, prediction=2.6258413791656494)\n |  >>> predictions[2]\n |  Row(user=2, item=0, prediction=-1.5018409490585327)\n |  >>> user_recs = model.recommendForAllUsers(3)\n |  >>> user_recs.where(user_recs.user == 0)        .select(\"recommendations.item\", \"recommendations.rating\").collect()\n |  [Row(item=[0, 1, 2], rating=[3.910..., 1.992..., -0.138...])]\n |  >>> item_recs = model.recommendForAllItems(3)\n |  >>> item_recs.where(item_recs.item == 2)        .select(\"recommendations.user\", \"recommendations.rating\").collect()\n |  [Row(user=[2, 1, 0], rating=[4.901..., 3.981..., -0.138...])]\n |  >>> user_subset = df.where(df.user == 2)\n |  >>> user_subset_recs = model.recommendForUserSubset(user_subset, 3)\n |  >>> user_subset_recs.select(\"recommendations.item\", \"recommendations.rating\").first()\n |  Row(item=[2, 1, 0], rating=[4.901..., 1.056..., -1.501...])\n |  >>> item_subset = df.where(df.item == 0)\n |  >>> item_subset_recs = model.recommendForItemSubset(item_subset, 3)\n |  >>> item_subset_recs.select(\"recommendations.user\", \"recommendations.rating\").first()\n |  Row(user=[0, 1, 2], rating=[3.910..., 2.625..., -1.501...])\n |  >>> als_path = temp_path + \"/als\"\n |  >>> als.save(als_path)\n |  >>> als2 = ALS.load(als_path)\n |  >>> als.getMaxIter()\n |  5\n |  >>> model_path = temp_path + \"/als_model\"\n |  >>> model.save(model_path)\n |  >>> model2 = ALSModel.load(model_path)\n |  >>> model.rank == model2.rank\n |  True\n |  >>> sorted(model.userFactors.collect()) == sorted(model2.userFactors.collect())\n |  True\n |  >>> sorted(model.itemFactors.collect()) == sorted(model2.itemFactors.collect())\n |  True\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  Method resolution order:\n |      ALS\n |      pyspark.ml.wrapper.JavaEstimator\n |      pyspark.ml.wrapper.JavaParams\n |      pyspark.ml.wrapper.JavaWrapper\n |      pyspark.ml.base.Estimator\n |      pyspark.ml.param.shared.HasCheckpointInterval\n |      pyspark.ml.param.shared.HasMaxIter\n |      pyspark.ml.param.shared.HasPredictionCol\n |      pyspark.ml.param.shared.HasRegParam\n |      pyspark.ml.param.shared.HasSeed\n |      pyspark.ml.param.Params\n |      pyspark.ml.util.Identifiable\n |      pyspark.ml.util.JavaMLWritable\n |      pyspark.ml.util.MLWritable\n |      pyspark.ml.util.JavaMLReadable\n |      pyspark.ml.util.MLReadable\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10, implicitPrefs=False, alpha=1.0, userCol='user', itemCol='item', seed=None, ratingCol='rating', nonnegative=False, checkpointInterval=10, intermediateStorageLevel='MEMORY_AND_DISK', finalStorageLevel='MEMORY_AND_DISK', coldStartStrategy='nan')\n |      __init__(self, rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10,                  implicitPrefs=false, alpha=1.0, userCol=\"user\", itemCol=\"item\", seed=None,                  ratingCol=\"rating\", nonnegative=false, checkpointInterval=10,                  intermediateStorageLevel=\"MEMORY_AND_DISK\",                  finalStorageLevel=\"MEMORY_AND_DISK\", coldStartStrategy=\"nan\")\n |  \n |  getAlpha(self)\n |      Gets the value of alpha or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getColdStartStrategy(self)\n |      Gets the value of coldStartStrategy or its default value.\n |      \n |      .. versionadded:: 2.2.0\n |  \n |  getFinalStorageLevel(self)\n |      Gets the value of finalStorageLevel or its default value.\n |      \n |      .. versionadded:: 2.0.0\n |  \n |  getImplicitPrefs(self)\n |      Gets the value of implicitPrefs or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getIntermediateStorageLevel(self)\n |      Gets the value of intermediateStorageLevel or its default value.\n |      \n |      .. versionadded:: 2.0.0\n |  \n |  getItemCol(self)\n |      Gets the value of itemCol or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getNonnegative(self)\n |      Gets the value of nonnegative or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getNumItemBlocks(self)\n |      Gets the value of numItemBlocks or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getNumUserBlocks(self)\n |      Gets the value of numUserBlocks or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getRank(self)\n |      Gets the value of rank or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getRatingCol(self)\n |      Gets the value of ratingCol or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getUserCol(self)\n |      Gets the value of userCol or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setAlpha(self, value)\n |      Sets the value of :py:attr:`alpha`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setColdStartStrategy(self, value)\n |      Sets the value of :py:attr:`coldStartStrategy`.\n |      \n |      .. versionadded:: 2.2.0\n |  \n |  setFinalStorageLevel(self, value)\n |      Sets the value of :py:attr:`finalStorageLevel`.\n |      \n |      .. versionadded:: 2.0.0\n |  \n |  setImplicitPrefs(self, value)\n |      Sets the value of :py:attr:`implicitPrefs`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setIntermediateStorageLevel(self, value)\n |      Sets the value of :py:attr:`intermediateStorageLevel`.\n |      \n |      .. versionadded:: 2.0.0\n |  \n |  setItemCol(self, value)\n |      Sets the value of :py:attr:`itemCol`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setNonnegative(self, value)\n |      Sets the value of :py:attr:`nonnegative`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setNumBlocks(self, value)\n |      Sets both :py:attr:`numUserBlocks` and :py:attr:`numItemBlocks` to the specific value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setNumItemBlocks(self, value)\n |      Sets the value of :py:attr:`numItemBlocks`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setNumUserBlocks(self, value)\n |      Sets the value of :py:attr:`numUserBlocks`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setParams(self, rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10, implicitPrefs=False, alpha=1.0, userCol='user', itemCol='item', seed=None, ratingCol='rating', nonnegative=False, checkpointInterval=10, intermediateStorageLevel='MEMORY_AND_DISK', finalStorageLevel='MEMORY_AND_DISK', coldStartStrategy='nan')\n |      setParams(self, rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10,                  implicitPrefs=False, alpha=1.0, userCol=\"user\", itemCol=\"item\", seed=None,                  ratingCol=\"rating\", nonnegative=False, checkpointInterval=10,                  intermediateStorageLevel=\"MEMORY_AND_DISK\",                  finalStorageLevel=\"MEMORY_AND_DISK\", coldStartStrategy=\"nan\")\n |      Sets params for ALS.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setRank(self, value)\n |      Sets the value of :py:attr:`rank`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setRatingCol(self, value)\n |      Sets the value of :py:attr:`ratingCol`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setUserCol(self, value)\n |      Sets the value of :py:attr:`userCol`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __slotnames__ = []\n |  \n |  alpha = Param(parent='undefined', name='alpha', doc='alpha for implici...\n |  \n |  coldStartStrategy = Param(parent='undefined', name='coldStartStrateg.....\n |  \n |  finalStorageLevel = Param(parent='undefined', name='finalStorageLevel'...\n |  \n |  implicitPrefs = Param(parent='undefined', name='implicitPrefs', doc='w...\n |  \n |  intermediateStorageLevel = Param(parent='undefined', name='intermediat...\n |  \n |  itemCol = Param(parent='undefined', name='itemCol', doc='c...ds. Ids m...\n |  \n |  nonnegative = Param(parent='undefined', name='nonnegative', do...to us...\n |  \n |  numItemBlocks = Param(parent='undefined', name='numItemBlocks', doc='n...\n |  \n |  numUserBlocks = Param(parent='undefined', name='numUserBlocks', doc='n...\n |  \n |  rank = Param(parent='undefined', name='rank', doc='rank of the factori...\n |  \n |  ratingCol = Param(parent='undefined', name='ratingCol', doc='column na...\n |  \n |  userCol = Param(parent='undefined', name='userCol', doc='c...ds. Ids m...\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n |  \n |  __metaclass__ = <class 'abc.ABCMeta'>\n |      Metaclass for defining Abstract Base Classes (ABCs).\n |      \n |      Use this metaclass to create an ABC.  An ABC can be subclassed\n |      directly, and then acts as a mix-in class.  You can also register\n |      unrelated concrete classes (even built-in classes) and unrelated\n |      ABCs as 'virtual subclasses' -- these and their descendants will\n |      be considered subclasses of the registering ABC by the built-in\n |      issubclass() function, but the registering ABC won't show up in\n |      their MRO (Method Resolution Order) nor will method\n |      implementations defined by the registering ABC be callable (not\n |      even via super()).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n |  \n |  copy(self, extra=None)\n |      Creates a copy of this instance with the same uid and some\n |      extra params. This implementation first calls Params.copy and\n |      then make a copy of the companion Java pipeline component with\n |      extra params. So both the Python wrapper and the Java pipeline\n |      component get copied.\n |      \n |      :param extra: Extra parameters to copy to the new instance\n |      :return: Copy of this instance\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __del__(self)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Estimator:\n |  \n |  fit(self, dataset, params=None)\n |      Fits a model to the input dataset with optional parameters.\n |      \n |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n |                     param maps is given, this calls fit on each param map and returns a list of\n |                     models.\n |      :returns: fitted model(s)\n |      \n |      .. versionadded:: 1.3.0\n |  \n |  fitMultiple(self, dataset, paramMaps)\n |      Fits a model to the input dataset for each param map in `paramMaps`.\n |      \n |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n |      :param paramMaps: A Sequence of param maps.\n |      :return: A thread safe iterable which contains one model for each param map. Each\n |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n |               using `paramMaps[index]`. `index` values may not be sequential.\n |      \n |      .. note:: DeveloperApi\n |      .. note:: Experimental\n |      \n |      .. versionadded:: 2.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n |  \n |  getCheckpointInterval(self)\n |      Gets the value of checkpointInterval or its default value.\n |  \n |  setCheckpointInterval(self, value)\n |      Sets the value of :py:attr:`checkpointInterval`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n |  \n |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n |  \n |  getMaxIter(self)\n |      Gets the value of maxIter or its default value.\n |  \n |  setMaxIter(self, value)\n |      Sets the value of :py:attr:`maxIter`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n |  \n |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  getPredictionCol(self)\n |      Gets the value of predictionCol or its default value.\n |  \n |  setPredictionCol(self, value)\n |      Sets the value of :py:attr:`predictionCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n |  \n |  getRegParam(self)\n |      Gets the value of regParam or its default value.\n |  \n |  setRegParam(self, value)\n |      Sets the value of :py:attr:`regParam`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n |  \n |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n |  \n |  getSeed(self)\n |      Gets the value of seed or its default value.\n |  \n |  setSeed(self, value)\n |      Sets the value of :py:attr:`seed`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n |  \n |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.Params:\n |  \n |  explainParam(self, param)\n |      Explains a single param and returns its name, doc, and optional\n |      default value and user-supplied value in a string.\n |  \n |  explainParams(self)\n |      Returns the documentation of all params with their optionally\n |      default values and user-supplied values.\n |  \n |  extractParamMap(self, extra=None)\n |      Extracts the embedded default param values and user-supplied\n |      values, and then merges them with extra values from input into\n |      a flat param map, where the latter value is used if there exist\n |      conflicts, i.e., with ordering: default param values <\n |      user-supplied values < extra.\n |      \n |      :param extra: extra param values\n |      :return: merged param map\n |  \n |  getOrDefault(self, param)\n |      Gets the value of a param in the user-supplied param map or its\n |      default value. Raises an error if neither is set.\n |  \n |  getParam(self, paramName)\n |      Gets a param by its name.\n |  \n |  hasDefault(self, param)\n |      Checks whether a param has a default value.\n |  \n |  hasParam(self, paramName)\n |      Tests whether this instance contains a param with a given\n |      (string) name.\n |  \n |  isDefined(self, param)\n |      Checks whether a param is explicitly set by user or has\n |      a default value.\n |  \n |  isSet(self, param)\n |      Checks whether a param is explicitly set by user.\n |  \n |  set(self, param, value)\n |      Sets a parameter in the embedded param map.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.param.Params:\n |  \n |  params\n |      Returns all params ordered by name. The default implementation\n |      uses :py:func:`dir` to get all attributes of type\n |      :py:class:`Param`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.Identifiable:\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n |  \n |  write(self)\n |      Returns an MLWriter instance for this ML instance.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.MLWritable:\n |  \n |  save(self, path)\n |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n |  \n |  read() from builtins.type\n |      Returns an MLReader instance for this class.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.MLReadable:\n |  \n |  load(path) from builtins.type\n |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# split into training and testing sets\n\ntraining,testing = movie_ratings.randomSplit([.8,.2])\n\n# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n\nals = ALS(maxIter = 5, rank = 4,userCol = 'userId', itemCol = 'movieId', coldStartStrategy = 'drop')\n\n# fit the ALS model to the training set\nmodel = als.fit(training)","execution_count":83,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you've fit the model, and it's time to evaluate it to determine just how well it performed.\n> \n* import `RegressionEvalutor` from pyspark.ml.evaluation\n* generate predictions with your model for the test set by using the `transform` method on your ALS model\n* evaluate your model and print out the RMSE from your test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing appropriate library\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Evaluate the model by computing the RMSE on the test data\n\npreds = model.transform(testing)\nevaluator = RegressionEvaluator(metricName = 'rmse', labelCol = 'rating', predictionCol = 'prediction')\n\nrmse = evaluator.evaluate(preds)\nrmse","execution_count":84,"outputs":[{"output_type":"execute_result","execution_count":84,"data":{"text/plain":"0.8993699392546267"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation to Find the Optimal Model\n\nLet's now find the optimal values for the parameters of the ALS model. Use the built-in Cross Validator in pyspark with a suitable param grid and determine the optimal model. Try with the parameters:\n\n* regularization = [0.01,0.001,0.1])\n* rank = [4,10,50]\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_ratings.head(5)","execution_count":85,"outputs":[{"output_type":"execute_result","execution_count":85,"data":{"text/plain":"[Row(userId=1, movieId=1, rating=4.0),\n Row(userId=1, movieId=3, rating=4.0),\n Row(userId=1, movieId=6, rating=4.0),\n Row(userId=1, movieId=47, rating=5.0),\n Row(userId=1, movieId=50, rating=5.0)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# initialize the ALS model\n\nals_1 = ALS(userCol = 'userId', itemCol= 'movieId', ratingCol = 'rating', coldStartStrategy = 'drop')\n\n# create the parameter grid              \n\nparams = ParamGridBuilder().addGrid(als_1.regParam, [.01, .001, .1]).addGrid(als_1.rank, [4,10,50]).build()\n\n## instantiating crossvalidator estimator\n\ncv = CrossValidator(estimator = als_1, estimatorParamMaps = params, evaluator = evaluator, parallelism = 4)\nbest = cv.fit(movie_ratings)\n# We see the best model has a rank of 50, so we will use that in our future models with this dataset\n\ntop_rank = best.bestModel.rank","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_rank","execution_count":87,"outputs":[{"output_type":"execute_result","execution_count":87,"data":{"text/plain":"50"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Incorporating the names of the movies\n\nWhen we make recommendations, it would be ideal if we could have the actual name of the movie be used rather than just an ID. There is another file called './data/movies.csv' that contains all of the names of the movies matched up to the movie_id that we have in the ratings dataset.\n\n* import the data into a Spark DataFrame\n* look at the first 5 rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_1 = '/kaggle/input/movie-lens-small-latest-dataset/movies.csv'","execution_count":88,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_titles = spark.read.csv(file_1, header = 'true', inferSchema = 'true')\n\nmovie_titles.head(5)","execution_count":89,"outputs":[{"output_type":"execute_result","execution_count":89,"data":{"text/plain":"[Row(movieId=1, title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy'),\n Row(movieId=2, title='Jumanji (1995)', genres='Adventure|Children|Fantasy'),\n Row(movieId=3, title='Grumpier Old Men (1995)', genres='Comedy|Romance'),\n Row(movieId=4, title='Waiting to Exhale (1995)', genres='Comedy|Drama|Romance'),\n Row(movieId=5, title='Father of the Bride Part II (1995)', genres='Comedy')]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We will eventually be matching up the movie_ids with the movie titles. In the cell below, create a function `name_retriever` that takes in a movie_id and returns a string that. \n\n> Hint: It's possible to do this operation in one line with the `df.where` or the `df.filter` method"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_titles.where(movie_titles.movieId == 1023).take(1)[0]['title']","execution_count":90,"outputs":[{"output_type":"execute_result","execution_count":90,"data":{"text/plain":"'Winnie the Pooh and the Blustery Day (1968)'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_ratings.head(5)","execution_count":91,"outputs":[{"output_type":"execute_result","execution_count":91,"data":{"text/plain":"[Row(movieId=1, title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy'),\n Row(movieId=2, title='Jumanji (1995)', genres='Adventure|Children|Fantasy'),\n Row(movieId=3, title='Grumpier Old Men (1995)', genres='Comedy|Romance'),\n Row(movieId=4, title='Waiting to Exhale (1995)', genres='Comedy|Drama|Romance'),\n Row(movieId=5, title='Father of the Bride Part II (1995)', genres='Comedy')]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def name_retriever(movie_id,movie_title_df):\n    title = movie_title_df.where(movie_title_df.movieId == movie_id).take(1)[0]['title']\n    return title","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(name_retriever(1023,movie_titles))","execution_count":93,"outputs":[{"output_type":"stream","text":"Winnie the Pooh and the Blustery Day (1968)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Getting Recommendations\n\nNow it's time to actually get some recommendations! The ALS model has built in methods called `recommendForUserSubset` and `recommendForAllUsers`. We'll start off with using a subset of users."},{"metadata":{"trusted":true},"cell_type":"code","source":"users = movie_ratings.select(als.getUserCol()).distinct().limit(1)\nuserSubsetRecs = model.recommendForUserSubset(users, 10)\nrecs = userSubsetRecs.take(1)","execution_count":94,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"\"cannot resolve '`userId`' given input columns: [movieId, title, genres];;\\n'Project ['userId]\\n+- Relation[movieId#8207,title#8208,genres#8209] csv\\n\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o11041.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`userId`' given input columns: [movieId, title, genres];;\n'Project ['userId]\n+- Relation[movieId#8207,title#8208,genres#8209] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-94-51286700d72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUserCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muserSubsetRecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommendForUserSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserSubsetRecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \"\"\"\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`userId`' given input columns: [movieId, title, genres];;\\n'Project ['userId]\\n+- Relation[movieId#8207,title#8208,genres#8209] csv\\n\""]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"recs[0][1]","execution_count":95,"outputs":[{"output_type":"execute_result","execution_count":95,"data":{"text/plain":"[Row(movieId=6818, rating=5.494475841522217),\n Row(movieId=4789, rating=5.1264801025390625),\n Row(movieId=8477, rating=5.039873123168945),\n Row(movieId=3379, rating=4.932860374450684),\n Row(movieId=136469, rating=4.907209396362305),\n Row(movieId=102217, rating=4.8987627029418945),\n Row(movieId=92494, rating=4.8987627029418945),\n Row(movieId=89904, rating=4.854540824890137),\n Row(movieId=148881, rating=4.851578235626221),\n Row(movieId=947, rating=4.824190139770508)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"recs","execution_count":96,"outputs":[{"output_type":"execute_result","execution_count":96,"data":{"text/plain":"[Row(userId=148, recommendations=[Row(movieId=6818, rating=5.494475841522217), Row(movieId=4789, rating=5.1264801025390625), Row(movieId=8477, rating=5.039873123168945), Row(movieId=3379, rating=4.932860374450684), Row(movieId=136469, rating=4.907209396362305), Row(movieId=102217, rating=4.8987627029418945), Row(movieId=92494, rating=4.8987627029418945), Row(movieId=89904, rating=4.854540824890137), Row(movieId=148881, rating=4.851578235626221), Row(movieId=947, rating=4.824190139770508)])]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We can now see we have a list of rows with recommended items. Now try and get the name of the top recommended movie by way of the function you just created, using number one item for this user."},{"metadata":{"trusted":true},"cell_type":"code","source":"# use indexing to obtain the movie id of top predicted rated item\nfirst_recommendation = recs[0]['recommendations'][0][0]\n\n# use the name retriever function to get the values\nname_retriever(first_recommendation,movie_titles)","execution_count":97,"outputs":[{"output_type":"execute_result","execution_count":97,"data":{"text/plain":"'Come and See (Idi i smotri) (1985)'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Of course, you can also make recommendations for everyone, although this will take longer. In the next line, we are creating an RDD with the top 5 recommendations for every user and then selecting one user to find out his predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"recommendations.where(recommendations.userId == 3).collect()[0][1][0][0]","execution_count":98,"outputs":[{"output_type":"execute_result","execution_count":98,"data":{"text/plain":"158783"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommendations = model.recommendForAllUsers(5)\nname_retriever(recommendations.where(recommendations.userId == 3).collect()[0][1][0][0], movie_titles)\n","execution_count":99,"outputs":[{"output_type":"execute_result","execution_count":99,"data":{"text/plain":"'Skin I Live In, The (La piel que habito) (2011)'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Getting Predictions for a New User\n\nNow, it's time to put together all that you've learned in this section to create a function that will take in a new user and some movies they've rated and then return n number of highest recommended movies. This function will have multiple different steps to it:\n\n* adding the new ratings into the dataframe (hint: look into using the union df method)\n* fitting the als model to\n* make recommendations for the user of choice\n* print out the names of the top n recommendations in a reader-friendly manner\n\nThe function should take in the parameters:\n* user_id : int \n* new_ratings : list of tuples in the format (user_id,item_id,rating)\n* rating_df : spark DF containing ratings\n* movie_title_df : spark DF containing movie titles\n* num_recs : int"},{"metadata":{},"cell_type":"markdown","source":"Rate these new movies\n\n```python\n[Row(movieId=3253, title=\"Wayne's World (1992)\", genres='Comedy'), - 2\n Row(movieId=2459, title='Texas Chainsaw Massacre, The (1974)', genres='Horror'), - 1\n Row(movieId=2513, title='Pet Sematary (1989)', genres='Horror'), - 2\n Row(movieId=6502, title='28 Days Later (2002)', genres='Action|Horror|Sci-Fi'), - 1\n Row(movieId=1091, title=\"Weekend at Bernie's (1989)\", genres='Comedy'), - 3\nRow(movieId=441, title='Dazed and Confused (1993)', genres='Comedy'), - 4\nRow(movieId=370, title='Naked Gun 33 1/3: The Final Insult (1994)', genres='Action|Comedy')] - 4\n\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_user_recs(user_id,new_ratings,rating_df,movie_title_df,num_recs):\n    # turn the new_recommendations list into a spark DataFrame\n    new_ratings = spark.createDataFrame(new_ratings, rating_df.columns)\n    \n    # combine the new ratings df with the rating_df\n    movie_ratings_updated = rating_df.union(new_ratings)\n    \n    # create an ALS model and fit it\n    als = ALS(maxIter = 5, rank = top_rank, regParam = .01, userCol = 'userId', itemCol = 'movieId', coldStartStrategy = 'drop')\n    \n    model = als.fit(movie_ratings_updated)\n    # make recommendations for all users using the recommendForAllUsers method\n\n    recommendations = model.recommendForAllUsers(num_recs)\n    # get recommendations specifically for the new user that has been added to the DataFrame\n    \n    print (recommendations)\n    \n    recs_for_new_user = recommendations.where(recommendations.userId == user_id).take(1)\n    \n    for ranking, (movie_id, rating) in enumerate(recs_for_new_user[0]['recommendations']):\n        title = name_retriever(movie_id, movie_title_df)\n        print ('Recommended {} - Movie: {}. Score = {}'.format(ranking + 1, title, rating))\n        ","execution_count":139,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_ratings.columns","execution_count":140,"outputs":[{"output_type":"execute_result","execution_count":140,"data":{"text/plain":"['userId', 'movieId', 'rating']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try out your function with the movies listed above\n\nuser_id = 100000\nratings_1 = [(user_id, 3253, 3),(user_id, 2459, 1), (user_id, 2513, 1), (user_id, 6502, 1), (user_id, 1091, 3) , (user_id, 441, 5)]\nnew_user_recs(user_id, new_ratings = ratings_1, rating_df = movie_ratings, movie_title_df = movie_titles, num_recs = 10)","execution_count":141,"outputs":[{"output_type":"stream","text":"DataFrame[userId: int, recommendations: array<struct<movieId:int,rating:float>>]\nRecommended 1 - Movie: Dazed and Confused (1993). Score = 4.988752841949463\nRecommended 2 - Movie: Sixteen Candles (1984). Score = 3.6716043949127197\nRecommended 3 - Movie: Ferris Bueller's Day Off (1986). Score = 3.632672071456909\nRecommended 4 - Movie: Young Frankenstein (1974). Score = 3.626786947250366\nRecommended 5 - Movie: Big Chill, The (1983). Score = 3.514853000640869\nRecommended 6 - Movie: Verdict, The (1982). Score = 3.4933922290802\nRecommended 7 - Movie: M*A*S*H (a.k.a. MASH) (1970). Score = 3.4788670539855957\nRecommended 8 - Movie: Goonies, The (1985). Score = 3.4779281616210938\nRecommended 9 - Movie: Mary Poppins (1964). Score = 3.4263033866882324\nRecommended 10 - Movie: Blood Simple (1984). Score = 3.423994541168213\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"So here we have it! Our recommendation system is generating recommendations for the top 10 movies. \n\n\n\n\n## Level up - Optional \n\n\n* Create a user interface to allow users to easily choose items and get recommendations.\n\n* Use IMDB links to scrape user reviews from IMDB and using basic NLP techniques, create extra embeddings for ALS model. \n\n* Create a hybrid recommender system using features like genre\n\n## Summarym\n\nIn this lab, we learned how to build a model using Spark, how to perform some parameter selection, and how to update the model every time that new user preferences come in. We looked at how Spark's ALS implementation can be be used to build a scalable and efficient recommendation system. We also saw that such systems can become computationally expensive and using them with an online system could be a problem with traditional computational platforms. Spark's distributed computing architecture provides a great solution to deploy such recommendation systems for real world applications (think Amazon, Spotify).\n\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}